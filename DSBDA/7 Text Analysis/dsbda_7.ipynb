{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NEETI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NEETI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\NEETI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NEETI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rapid development of artificial intelligence (ai) and machine learning technologies has led to significant advancements in various industries, from healthcare to finance. ai-powered applications are being used to analyze large datasets, make predictions, and improve decision-making processes. however, with these advancements come concerns about data privacy, ethical implications, and the potential for job displacement. as businesses continue to adopt ai, it is crucial for policymakers and industry leaders to collaborate in creating guidelines that ensure responsible and equitable implementation of these technologies.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting text from a text file and converting into lowercase\n",
    "with open('sample_text.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document\n",
    "document = \"\"\"Text analytics is a process of extracting meaningful insights from textual data. \n",
    "It involves techniques such as tokenization, stemming, lemmatization, and stop words removal.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Words:\n",
      "['Text', 'analytics', 'is', 'a', 'process', 'of', 'extracting', 'meaningful', 'insights', 'from', 'textual', 'data', '.', 'It', 'involves', 'techniques', 'such', 'as', 'tokenization', ',', 'stemming', ',', 'lemmatization', ',', 'and', 'stop', 'words', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "### **Step 1: Tokenization**\n",
    "tokens = word_tokenize(document)\n",
    "print(\"\\nTokenized Words:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part-of-Speech Tagging:\n",
      "[('Text', 'NN'), ('analytics', 'NNS'), ('is', 'VBZ'), ('a', 'DT'), ('process', 'NN'), ('of', 'IN'), ('extracting', 'VBG'), ('meaningful', 'JJ'), ('insights', 'NNS'), ('from', 'IN'), ('textual', 'JJ'), ('data', 'NNS'), ('.', '.'), ('It', 'PRP'), ('involves', 'VBZ'), ('techniques', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('tokenization', 'NN'), (',', ','), ('stemming', 'VBG'), (',', ','), ('lemmatization', 'NN'), (',', ','), ('and', 'CC'), ('stop', 'VB'), ('words', 'NNS'), ('removal', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "### **Step 2: POS Tagging**\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nPart-of-Speech Tagging:\")\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix:\n",
      "      brown      but       dog       fox      jump     jumps      lazy  \\\n",
      "0  0.414223  0.00000  0.244647  0.315027  0.000000  0.414223  0.244647   \n",
      "1  0.000000  0.00000  0.274634  0.000000  0.464997  0.000000  0.274634   \n",
      "2  0.000000  0.34816  0.205629  0.264785  0.000000  0.000000  0.205629   \n",
      "\n",
      "      never      over     quick   quickly       the       was  \n",
      "0  0.000000  0.315027  0.315027  0.000000  0.489294  0.000000  \n",
      "1  0.464997  0.353642  0.000000  0.464997  0.274634  0.000000  \n",
      "2  0.000000  0.000000  0.264785  0.000000  0.411258  0.696321  \n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Never jump over the lazy dog quickly\",\n",
    "    \"The dog was lazy but the fox was quick\"\n",
    "]\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame for better display\n",
    "import pandas as pd\n",
    "df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
